{"version":3,"sources":["client.ts"],"names":["OllamaClient","BackendClient","create","createOllama","settings","baseURL","getEnv","fetch","vercelFetcher"],"mappings":";;;;;;;;;AAuBO,MAAMA,qBAAqBC,wBAAAA,CAAAA;EAvBlC;;;EAwBYC,MAAyB,GAAA;AACjC,IAAA,OAAOC,6BAAa,CAAA;MAClB,GAAI,IAAA,CAAKC,YAAY,EAAC;AACtBC,MAAAA,OAAAA,EAAS,IAAKD,CAAAA,QAAAA,EAAUC,OAAWC,IAAAA,cAAAA,CAAO,iBAAA,CAAA;MAC1CC,KAAOC,EAAAA,uBAAAA,CAAc,IAAKJ,CAAAA,QAAAA,EAAUG,KAAAA;KACtC,CAAA;AACF;AACF","file":"client.cjs","sourcesContent":["/**\n * Copyright 2025 Â© BeeAI a Series of LF Projects, LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { getEnv } from \"@/internals/env.js\";\nimport { createOllama, OllamaProvider, OllamaProviderSettings } from \"ollama-ai-provider\";\nimport { BackendClient } from \"@/backend/client.js\";\nimport { vercelFetcher } from \"@/adapters/vercel/backend/utils.js\";\n\nexport type OllamaClientSettings = OllamaProviderSettings;\n\nexport class OllamaClient extends BackendClient<OllamaClientSettings, OllamaProvider> {\n  protected create(): OllamaProvider {\n    return createOllama({\n      ...(this.settings ?? {}),\n      baseURL: this.settings?.baseURL ?? getEnv(\"OLLAMA_BASE_URL\"),\n      fetch: vercelFetcher(this.settings?.fetch),\n    });\n  }\n}\n"]}