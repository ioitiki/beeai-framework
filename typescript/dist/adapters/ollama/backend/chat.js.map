{"version":3,"sources":["chat.ts"],"names":["OllamaChatModel","VercelChatModel","supportsToolStreaming","toolChoiceSupport","constructor","modelId","getEnv","settings","client","model","OllamaClient","ensure","instance","chat","structuredOutputs","register"],"mappings":";;;;;;AA0BO,MAAMA,wBAAwBC,eAAAA,CAAAA;EA1BrC;;;EA2BWC,qBAAwB,GAAA,KAAA;EACjBC,iBAAkD,GAAA;AAAC,IAAA,MAAA;AAAQ,IAAA;;EAE3EC,WACEC,CAAAA,OAAAA,GAA6BC,OAAO,mBAAqB,EAAA,aAAA,GACzDC,QAAoC,GAAA,IACpCC,MACA,EAAA;AACA,IAAA,MAAMC,QAAQC,YAAaC,CAAAA,MAAAA,CAAOH,MAAAA,CAAQI,CAAAA,QAAAA,CAASC,KAAKR,OAAS,EAAA;MAC/D,GAAGE,QAAAA;MACHO,iBAAmB,EAAA;KACrB,CAAA;AACA,IAAA,KAAA,CAAML,KAAAA,CAAAA;AACR;EAEA;AACE,IAAA,IAAA,CAAKM,QAAQ,EAAA;AACf;AACF","file":"chat.js","sourcesContent":["/**\n * Copyright 2025 Â© BeeAI a Series of LF Projects, LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { VercelChatModel } from \"@/adapters/vercel/backend/chat.js\";\nimport { OllamaProvider } from \"ollama-ai-provider\";\nimport { OllamaClient, OllamaClientSettings } from \"@/adapters/ollama/backend/client.js\";\nimport { getEnv } from \"@/internals/env.js\";\nimport { ChatModelToolChoiceSupport } from \"@/backend/chat.js\";\n\ntype OllamaParameters = Parameters<OllamaProvider[\"languageModel\"]>;\nexport type OllamaChatModelId = NonNullable<OllamaParameters[0]>;\nexport type OllamaChatModelSettings = NonNullable<OllamaParameters[1]>;\n\nexport class OllamaChatModel extends VercelChatModel {\n  readonly supportsToolStreaming = false;\n  public readonly toolChoiceSupport: ChatModelToolChoiceSupport[] = [\"none\", \"auto\"];\n\n  constructor(\n    modelId: OllamaChatModelId = getEnv(\"OLLAMA_CHAT_MODEL\", \"llama3.1:8b\"),\n    settings: OllamaChatModelSettings = {},\n    client?: OllamaClient | OllamaClientSettings,\n  ) {\n    const model = OllamaClient.ensure(client).instance.chat(modelId, {\n      ...settings,\n      structuredOutputs: true, // otherwise breaks generated structure\n    });\n    super(model);\n  }\n\n  static {\n    this.register();\n  }\n}\n"]}